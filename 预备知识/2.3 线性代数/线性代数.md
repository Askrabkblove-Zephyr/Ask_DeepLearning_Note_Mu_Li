## 2.3. 线性代数

### 2.3.1. 标量

标量就是0维张量,下方是标量的一些计算公式

<img src="pics/image-20251202143714077.png" alt="image-20251202143714077" style="zoom:50%;" />

### 2.3.2. 向量

下方是向量的一些计算公式和不等式

<img src="pics/image-20251202151942894.png" alt="image-20251202151942894" style="zoom:50%;" />

向量运算示意图

<img src="pics/image-20251202153115101.png" alt="image-20251202153115101" style="zoom:50%;" />

<img src="pics/image-20251202153151171.png" alt="image-20251202153151171" style="zoom:50%;" />



<img src="pics/image-20251202153416083.png" alt="image-20251202153416083" style="zoom:50%;" />



<img src="pics/image-20251202153007600.png" alt="image-20251202153007600" style="zoom:50%;" />

向量可以被视为标量值组成的列表。

```python
x = torch.arange(4)
x
```

#### 2.3.2.1. 长度、维度和形状

```
len(x) # len()函数来访问张量的长度
x.shape # 也可以通过.shape属性访问向量的长度
```

### 2.3.3. 矩阵

<img src="pics/image-20251202153513006.png" alt="image-20251202153513006" style="zoom:50%;" />

矩阵乘法：行*列

<img src="pics/image-20251202153547896.png" alt="image-20251202153547896" style="zoom:50%;" />

矩阵作用在向量上，相当于把向量扭曲了

<img src="pics/image-20251202153628160.png" alt="image-20251202153628160" style="zoom:50%;" />

矩阵乘法

<img src="pics/image-20251202153704750.png" alt="image-20251202153704750" style="zoom:50%;" />

<img src="pics/image-20251202153732206.png" alt="image-20251202153732206" style="zoom:50%;" />

Frobenius 范数就是把矩阵拉成一个一维向量，然后每个元素求平方后加起来最后开根号

<img src="pics/image-20251202153923965.png" alt="image-20251202153923965" style="zoom:67%;" />

<img src="pics/image-20251202154212037-1764661364962-1.png" alt="image-20251202154212037" style="zoom: 50%;" />

<img src="pics/image-20251202154914004.png" alt="image-20251202154914004" style="zoom:50%;" />

<img src="pics/image-20251202154944649.png" alt="image-20251202154944649" style="zoom:50%;" />

```python
A = torch.arange(20).reshape(5, 4) # 创建 5行4列20个元素的矩阵
A.T # 打印A的转置矩阵
```

### 2.3.4. 张量

```python
A = torch.arange(24).reshape(2, 3, 4)
A
```

输出结果：

```python
tensor([[[ 0,  1,  2,  3],
         [ 4,  5,  6,  7],
         [ 8,  9, 10, 11]],

        [[12, 13, 14, 15],
         [16, 17, 18, 19],
         [20, 21, 22, 23]]])
```

练习示例：

```python
import numpy as np
A = np.array([1, 2, 3]) 
print(A.shape)
print(A.ndim)
A
```

输出如下：

```python
(3,) # 这是一个一维数组（元组只有一个元素）
1 # 一维
[1 2 3]
```

若是2维就可以看到效果：

```python
import numpy as np
A = np.array([[1, 2, 3]]) # 表示在每个维度上的大小
print(A.shape)
print(A.ndim)
A.T
```

<img src="pics/image-20251202162327910.png" alt="image-20251202162327910" style="zoom: 67%;" />

clone 是重新再分配新的内存，如果直接 `A = B` 那么不会分配新内存

```python
A = torch.arange(20, dtype=torch.float32).reshape(5, 4)
B = A.clone()  # 通过分配新内存，将A的一个副本分配给B
A, A + B
```

#### 哈达玛积和矩阵乘法：

<img src="pics/image-20251202162941716.png" alt="image-20251202162941716" style="zoom: 67%;" />

```python
A = torch.arange(16, dtype=torch.float32).reshape(4, 4)
B = A.clone()

# 哈达玛积（逐元素相乘）
hadamard = A * B
print("\n哈达玛积 A * B:\n", hadamard)

# 矩阵乘法
matmul = A @ B
print("\n矩阵乘法 A @ B:\n", matmul)
```

将张量乘以或加上一个标量不会改变张量的形状，其中张量的每个元素都将与标量相加或相乘。

```python
a = 2
X = torch.arange(24).reshape(2, 3, 4)
a + X, (a * X).shape # X所有元素+a，X所有元素*a
```

### 2.3.6. 降维

##### sum 求元素和

```python
x = torch.arange(4, dtype=torch.float32)
x, x.sum()
```

```python
A = torch.arange(40, dtype=torch.float32).reshape(2, 5, 4)
A.shape, A.sum()
```

##### 维度求和 

3D：

​	`axis = 0` 矩阵堆叠

​	`axis = 1` 行合并

​	`axis = 2` 列合并

2D：

​	`axis = 0` 行合并

​	`axis = 1` 列合并

**矩阵堆叠**：

```python
A_sum_axis0 = A.sum(axis=0)
A_sum_axis0, A_sum_axis0.shape
```

```
(tensor([[20., 22., 24., 26.],
         [28., 30., 32., 34.],
         [36., 38., 40., 42.],
         [44., 46., 48., 50.],
         [52., 54., 56., 58.]]),
 torch.Size([5, 4]))
```

**行合并**：

```python
A_sum_axis1 = A.sum(axis=1)
A_sum_axis1, A_sum_axis1.shape
```

```python
(tensor([[ 40.,  45.,  50.,  55.],
         [140., 145., 150., 155.]]),
 torch.Size([2, 4])) #行没了
```

**列合并**：

```python
A_sum_axis1 = A.sum(axis=2)
A_sum_axis1, A_sum_axis1.shape
```

```python
(tensor([[  6.,  22.,  38.,  54.,  70.],
         [ 86., 102., 118., 134., 150.]]),
 torch.Size([2, 5])) #  #列没了
```

**沿着行和列对矩阵求和，等价于对矩阵的所有元素进行求和**。

```python
A.sum(axis=[0, 1, 2])  # 结果和A.sum()相同
```

一个与求和相关的量是*平均值*（mean或average）。 我们通过将总和除以元素总数来计算平均值。 在代码中，我们可以调用函数来计算任意形状张量的平均值。

```python
A.mean(), A.sum() / A.numel(), A.sum() / len(A)
print(A.shape, A.numel(), len(A))
```

跑一下就知道了，我们看结果：

```python
torch.Size([2, 5, 4]) 40 2
```

同样，计算平均值的函数也可以沿指定轴降低张量的维度。

```python
A.mean(axis=0), A.sum(axis=0) / A.shape[0]
print(A.shape, A.shape[0], A.shape[1], A.shape[2])
```

输出结果：

```
torch.Size([2, 5, 4]) 2 5 4
```

### 2.3.6.1. 非降维求和

但是，有时在调用函数来计算总和或均值时保持轴数不变会很有用。

```python
sum_A = A.sum(axis=1, keepdims=True)
print(sum_A, sum_A.shape)
print(A.shape)
print(A)

sum_A = A.sum(axis=1, keepdims=False)
print(sum_A, sum_A.shape)
print(A.shape)
print(A)
```

两段代码对照跑一下就发现确实不一样，keepdims保证了sum_A的轴数不变

```
A / sum_A # 广播机制
```

**累加求和**：

```
原始矩阵 B:
[[1 2 3]
 [4 5 6]
 [7 8 9]]

B.cumsum(axis=0):
[[ 1  2  3]   # 第0行保持不变
 [ 5  7  9]   # 第0行 + 第1行: [1+4, 2+5, 3+6]
 [12 15 18]]  # 前两行 + 第2行: [5+7, 7+8, 9+9]
```

### 2.3.7. 点积（Dot Product）

<img src="pics/image-20251202182842565.png" alt="image-20251202182842565" style="zoom: 67%;" />

```python
y = torch.ones(4, dtype = torch.float32)
x, y, torch.dot(x, y)
```

<img src="pics/image-20251202183143388.png" alt="image-20251202183143388" style="zoom:80%;" />

注意，我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积：

```python
torch.sum(x * y)
```

## 2.3.8. 矩阵-向量积

```python
import torch

# 假设 A 是矩阵，x 是向量
A = torch.tensor([[1., 2., 3.],
                  [4., 5., 6.]])  # 2×3矩阵
x = torch.tensor([1., 0., 2.])    # 3维向量

print("A.shape:", A.shape)      # torch.Size([2, 3])
print("x.shape:", x.shape)      # torch.Size([3])，一维张量/数组，有3个元素

# torch.mv() 是专门的矩阵-向量乘法函数
result = torch.mv(A, x)
print("torch.mv(A, x):", result)
print("result.shape:", result.shape)
```

`torch.Size([3])` 等价于`(3，)` ,**PyTorch便利性**：允许 `(2,3) × (3,)`，自动把一维向量当作合适的列/行向量

### 2.3.9. 矩阵-矩阵乘法

### 2.3.10. 范数

<img src="pics/image-20251202192440828.png" alt="image-20251202192440828" style="zoom:50%;" />

```python
u = torch.tensor([3.0, -4.0])
torch.norm(u)
```

![image-20251202192613793](pics/image-20251202192613793.png)

```python
torch.abs(u).sum()
```

![image-20251202192829312](pics/image-20251202192829312.png)

![image-20251202192843585](pics/image-20251202192843585.png)

```python
torch.norm(torch.ones((4, 9))) # 把矩阵拉成一维向量
```

